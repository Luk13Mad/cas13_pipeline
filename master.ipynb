{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3bb761-c864-455c-aba1-d46d6ca615aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,shutil,subprocess,sys,pickle\n",
    "import itertools as itt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5218544-5a56-4783-8ad0-3cf4cee03b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust this to your needs \n",
    "#the current form was written for IBM Spectrum LSF Standard 10.1.0.12\n",
    "def HPC_wrapper(command : str, queue : str = \"verylong\" , memory : int = 20, cores : int = 1) -> str:\n",
    "    first_part =   f\"bsub -R \\\"rusage[mem={memory}G] span[hosts=1] affinity[core({cores})]\\\" -q {queue} -o {WORKING_DIR}logs \\\" \"\n",
    "    last_part = \" \\\" \\n\"\n",
    "    \n",
    "    return first_part + command + last_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd17ff00-077b-48a4-84ae-d6e44c4eb36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory with folder structure produced by setup.sh\n",
    "WORKING_DIR = \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01837cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "below is the minimum structure required for the pipeline\n",
    "\n",
    "WORKING_DIR\n",
    "├── dLFC_analysis\n",
    "├── GEMINI_input\n",
    "├── processed_sequencing_data\n",
    "├── raw_sequencing_data\n",
    "├── Snakefile\n",
    "└── scripts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cd1760-bca8-4bb3-b9df-42b911430c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cebc42e-4c38-440f-b407-b98cd34373a2",
   "metadata": {},
   "source": [
    "The following assumes you already ran the Snakemake pipeline and obtained the files with the counts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "245ac739-7f6c-447c-945e-fae70c215e54",
   "metadata": {},
   "source": [
    "# Collect the count results \n",
    "\n",
    "Collect and aggreagte the count results file produced by snakemake pipeline from the processed_sequencing_data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b7d629-993c-44ea-a353-526305d8bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with names of folder you wish to collect the count results from\n",
    "subfolders_with_countresults = [\"RPE_2Dscreen\",\"T98G_2DScreen\",\"X204SC20113232-Z01-F010\",\"X204SC20113232-Z01-F007\"]\n",
    "celllines = [\"RPE\",\"T98G\",\"PATU\",\"HEK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28621894-bbda-4a9b-bcfe-74096bda7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the aggregate_counts for each subfolder\n",
    "for folder in subfolders_with_countresults:\n",
    "    gemini_output_folder = WORKING_DIR+\"GEMINI_input/\"+folder+\"/\"\n",
    "    if not os.path.exists(gemini_output_folder):\n",
    "        try:\n",
    "            os.makedirs(gemini_output_folder)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "          \n",
    "    #run once for raw counts\n",
    "    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/aggregate_counts.py\", #call script\n",
    "                    \"--directory\",folder, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--outputdir\",gemini_output_folder, #path where results will be written to\n",
    "                   ])\n",
    "    \n",
    "    #run once for counts as CPM\n",
    "    '''\n",
    "    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/aggregate_counts.py\", #call script\n",
    "                    \"--directory\",folder, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--outputdir\",gemini_output_folder, #path where results will be written to\n",
    "                    \"--normalize\",\n",
    "                   ])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498c66c-253f-4a3c-a7ce-d59fbbffa73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges the results from the previous steps and removes redundant files\n",
    "for folder,cellline in zip(subfolders_with_countresults,celllines):\n",
    "    counts_folder = WORKING_DIR+\"GEMINI_input/\"+folder+\"/\"\n",
    "    \n",
    "    #aggregate raw counts\n",
    "    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/merge_counts.py\", #call script\n",
    "                    \"--cellline\",cellline, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--counts_dir\",counts_folder, #path where results will be written to\n",
    "                   ])\n",
    "    \n",
    "    #aggregate normalized counts\n",
    "    '''\n",
    "    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/merge_counts.py\", #call script\n",
    "                    \"--cellline\",cellline, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--counts_dir\",counts_folder, #path where results will be written to\n",
    "                    \"--normalize\",\n",
    "                   ])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94389789-7ba7-446f-93cd-a75e282de79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you need to rename the columns to something more usefull do it now\n",
    "files_raw = [\"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/RPE_2Dscreen/merged_counts_RPE_raw.csv\",\n",
    " \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/T98G_2DScreen/merged_counts_T98G_raw.csv\",\n",
    " \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/X204SC20113232-Z01-F010/merged_counts_PATU_raw.csv\",\n",
    " \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/X204SC20113232-Z01-F007/merged_counts_HEK_raw.csv\"]\n",
    "\n",
    "files_norm = [\"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/RPE_2Dscreen/merged_counts_RPE_norm.csv\",\n",
    " \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/T98G_2DScreen/merged_counts_T98G_norm.csv\",\n",
    " \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/X204SC20113232-Z01-F010/merged_counts_PATU_norm.csv\",\n",
    " \"/b06x-isi/b062/a-c/Braun_CRISPR_2D/GEMINI_input/X204SC20113232-Z01-F007/merged_counts_HEK_norm.csv\"]\n",
    "\n",
    "rename_dicts = [{\"RPE_FF6\":\"RPETP3.REP2\", \"RPE_AA1\":\"RPETP1.REP1\", \"RPE_EE5\":\"RPETP2.REP2\", \"RPE_DD4\":\"RPETP1.REP2\", \"RPE_CC3\":\"RPETP3.REP1\", \"RPE_BB2\":\"RPETP2.REP1\"},\n",
    "               {\"s0713_BB2_2_5\":\"T98GTP2.REP1\",\"s0713_DD4_1_7\":\"T98GTP1.REP2\",\"s0713_AA1_1_4\":\"T98GTP1.REP1\",\"s0713_CC3_3_6\":\"T98GTP3.REP1\",\"s0713_FF6_3_5\":\"T98GTP3.REP2\",\"s0713_EE5_2_4\":\"T98GTP2.REP2\"},\n",
    "               {\"PA_TU8988_D4\":\"PATUTP1.REP2\",\"PA_TU8988_F6\":\"PATUTP3.REP2\",\"PA_TU8988_B2\":\"PATUTP2.REP1\",\"PA_TU8988_E5\":\"PATUTP2.REP2\",\"PA_TU8988_A1\":\"PATUTP1.REP1\",\"PA_TU8988_C3\":\"PATUTP3.REP1\"},\n",
    "               {\"C3_3_6\":\"HEKTP2.REP1\",\"E5_2_4\":\"HEKTP3.REP2\",\"F6_3_5\":\"HEKTP2.REP2\",\"D4_1_7\":\"HEKTP1.REP2\",\"A1_1_4\":\"HEKTP1.REP1\",\"B2_2_5\":\"HEKTP3.REP1\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7675e5-935f-44a3-a447-1b75eb5bf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually renaming columns\n",
    "for f,d in zip(files_raw,rename_dicts):\n",
    "    df = pd.read_csv(f,sep=\"\\t\",index_col=0)\n",
    "    df = df.rename(d,axis=1)\n",
    "    df.to_csv(f,sep=\"\\t\")\n",
    "    \n",
    "for f,d in zip(files_norm,rename_dicts):\n",
    "    df = pd.read_csv(f,sep=\"\\t\",index_col=0)\n",
    "    df = df.rename(d,axis=1)\n",
    "    df.to_csv(f,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc7ee9-09b8-4f4b-8215-8d2e453fd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy merged count results into dLFC folder\n",
    "for src in glob.glob(WORKING_DIR+\"GEMINI_input/*/merged_counts_*.csv\"):\n",
    "    name = src.split(\"/\")[-1]\n",
    "    dst = WORKING_DIR+\"dLFC_analysis/\"+name\n",
    "    try:\n",
    "        shutil.copyfile(src,dst)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53871529-afb4-4219-88a4-b8d82648edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy toml files into dLFC folder\n",
    "for src in glob.glob(WORKING_DIR+\"GEMINI_input/*/config_*.toml\"):\n",
    "    name = src.split(\"/\")[-1]\n",
    "    dst = WORKING_DIR+\"dLFC_analysis/\"+name\n",
    "    try:\n",
    "        shutil.copyfile(src,dst)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd3476c-8718-472e-97cc-ea96b018c1d0",
   "metadata": {},
   "source": [
    "# Run GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568c61c-8d48-44c2-966c-6ae1718820b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_anno_file = WORKING_DIR + \"metainfo/guide_anno.csv\"\n",
    "path_sample_anno = WORKING_DIR + \"metainfo/sample_replicate_anno.csv\"\n",
    "path_exclusion_file = WORKING_DIR + \"metainfo/combinations_affected_by_TTTTgate.tsv\"\n",
    "\n",
    "with open(WORKING_DIR + \"submission_script.sh\",\"w+\") as file:\n",
    "    file.write(\"module load R/4.1.0\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    for folder,cellline in zip(subfolders_with_countresults,celllines):\n",
    "        if cellline != \"PATU\":\n",
    "            continue\n",
    "        path_counts_file = WORKING_DIR + f\"GEMINI_input/{folder}/merged_counts_{cellline}_raw.csv\"\n",
    "        path_config_file = WORKING_DIR + f\"GEMINI_input/{folder}/config_{cellline}.toml\"\n",
    "    \n",
    "        command = \" \".join([\"Rscript\",WORKING_DIR+\"scripts/gemini_single.R\", #call script\n",
    "                            path_anno_file,\n",
    "                            path_sample_anno,\n",
    "                            path_counts_file, \n",
    "                            path_exclusion_file,\n",
    "                            str(4),\n",
    "                            WORKING_DIR,\n",
    "                            path_config_file\n",
    "                           ])\n",
    "    \n",
    "        file.write(HPC_wrapper(command = command,queue= \"medium\" , memory= 10, cores= 6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94b90683-9225-458b-b02e-a80660fe9592",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run dLFC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e07f98e1-0330-472b-9034-4b7cc6d785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing LFC values from raw count values\n",
    "for path_count_file in glob.glob(WORKING_DIR+\"dLFC_analysis/merged_counts_*_raw.csv\"):\n",
    "    cellline = path_count_file.split(\"/\")[-1].split(\"_\")[2]\n",
    "\n",
    "    path_config_file = WORKING_DIR+\"dLFC_analysis/config_{}.toml\".format(cellline)\n",
    "    \n",
    "    output_dir = WORKING_DIR + \"dLFC_analysis/\"\n",
    "    path_anno_file = WORKING_DIR + \"metainfo/guide_anno.csv\"\n",
    "    path_exclusion_file = WORKING_DIR + \"metainfo/combinations_affected_by_TTTTgate.tsv\"\n",
    "    path_sample_anno = WORKING_DIR + \"metainfo/sample_replicate_anno.csv\"\n",
    "    \n",
    "    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/prepare_raw_LFC.py\", #call script\n",
    "                    \"--output_dir\",output_dir, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--path_count_file\",path_count_file, #path to counts files\n",
    "                    \"--path_config_file\",path_config_file,\n",
    "                    \"--path_anno_file\",path_anno_file,\n",
    "                    \"--path_exclusion_file\",path_exclusion_file,\n",
    "                    \"--path_sample_anno\",path_sample_anno,\n",
    "                   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8bfc3a3-9481-409a-8dc9-829d98636407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor path_count_file in glob.glob(WORKING_DIR+\"dLFC_analysis/merged_counts_*.csv\"):\\n    cellline = path_count_file.split(\"/\")[-1].split(\"_\")[2]\\n\\n    path_config_file = WORKING_DIR+\"dLFC_analysis/config_{}.toml\".format(cellline)\\n    \\n    output_dir = WORKING_DIR + \"dLFC_analysis/\"\\n    path_anno_file = WORKING_DIR + \"metainfo/guide_anno.csv\"\\n    path_exclusion_file = WORKING_DIR + \"metainfo/combinations_affected_by_TTTTgate.tsv\"\\n    path_sample_anno = WORKING_DIR + \"metainfo/sample_replicate_anno.csv\"\\n    \\n    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/process_raw_LFC.py\", #call script\\n                    \"--output_dir\",output_dir, #setting the subfolder count results will be extracted from\\n                    \"--workingdir\",WORKING_DIR, #passing working dir to script\\n                    \"--cellline\",path_count_file, #path to counts files\\n                    \"--comparison\",path_config_file,\\n                    \"--lfc_file\",path_anno_file,\\n                   ])\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processing the LFC values to obtain expected and observed values\n",
    "#this step is recommended to be run in some sort of HPC cluster\n",
    "comparisons = [\"12\",\"13\",\"23\"]\n",
    "\n",
    "with open(WORKING_DIR + \"submission_script.sh\",\"w+\") as file:\n",
    "    file.write(f\"cd {WORKING_DIR}dLFC_analysis \\n\")\n",
    "    file.write(\"module load anaconda3/2021.05 \\n\")\n",
    "    file.write(\"source activate cas13_pipeline \\n\")\n",
    "    \n",
    "    for cellline in celllines:\n",
    "        for comp in comparisons:\n",
    "            lfc_file = WORKING_DIR+\"dLFC_analysis/raw_LFCZ_{}_{}.tsv\".format(cellline,comp)\n",
    "    \n",
    "            output_dir = WORKING_DIR + \"dLFC_analysis/\"\n",
    "        \n",
    "            command = \" \".join([\"python\",WORKING_DIR+\"scripts/process_raw_LFC.py\", #call script\n",
    "                    \"--output_dir\",output_dir, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--cellline\",cellline, #path to counts files\n",
    "                    \"--comparison\",comp,\n",
    "                    \"--lfc_file\",lfc_file,\n",
    "                   ])\n",
    "    \n",
    "            file.write(HPC_wrapper(command))\n",
    "\n",
    "\n",
    "#however if you are comfortable using it in the context of this notebook use the code below\n",
    "'''\n",
    "for path_count_file in glob.glob(WORKING_DIR+\"dLFC_analysis/merged_counts_*.csv\"):\n",
    "    cellline = path_count_file.split(\"/\")[-1].split(\"_\")[2]\n",
    "\n",
    "    path_config_file = WORKING_DIR+\"dLFC_analysis/config_{}.toml\".format(cellline)\n",
    "    \n",
    "    output_dir = WORKING_DIR + \"dLFC_analysis/\"\n",
    "    path_anno_file = WORKING_DIR + \"metainfo/guide_anno.csv\"\n",
    "    path_exclusion_file = WORKING_DIR + \"metainfo/combinations_affected_by_TTTTgate.tsv\"\n",
    "    path_sample_anno = WORKING_DIR + \"metainfo/sample_replicate_anno.csv\"\n",
    "    \n",
    "    subprocess.run([f\"{sys.executable}\",WORKING_DIR+\"scripts/process_raw_LFC.py\", #call script\n",
    "                    \"--output_dir\",output_dir, #setting the subfolder count results will be extracted from\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--cellline\",path_count_file, #path to counts files\n",
    "                    \"--comparison\",path_config_file,\n",
    "                    \"--lfc_file\",path_anno_file,\n",
    "                   ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2cbf7da-4fa0-4739-81b1-db40b7bd8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run dLFC rank test\n",
    "all_cellline_TP_combis = [[\"HEK\",\"12\"],[\"HEK\",\"13\"],[\"HEK\",\"23\"],\n",
    "         [\"T98G\",\"12\"],[\"T98G\",\"13\"],[\"T98G\",\"23\"],\n",
    "         [\"PATU\",\"12\"],[\"PATU\",\"13\"],[\"PATU\",\"23\"],\n",
    "         [\"RPE\",\"12\"],[\"RPE\",\"13\"],[\"RPE\",\"23\"]]\n",
    "\n",
    "with open(WORKING_DIR + \"submission_script.sh\",\"w+\") as file:\n",
    "    file.write(f\"cd {WORKING_DIR}dLFC_analysis \\n\")\n",
    "    file.write(\"module load anaconda3/2021.05 \\n\")\n",
    "    file.write(\"source activate cas13_pipeline \\n\")\n",
    "    \n",
    "    for cellline,comparison in all_cellline_TP_combis:\n",
    "        \n",
    "        command = \" \".join([\"python\",WORKING_DIR+\"scripts/dLFC_ranktest.py\", #call script\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--cellline\",cellline, #path to counts files\n",
    "                    \"--comparison\",comparison, #comparison of timepoints\n",
    "                   ])\n",
    "    \n",
    "        file.write(HPC_wrapper(command))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fa45adc-2590-4b19-a535-1d8134009308",
   "metadata": {
    "tags": []
   },
   "source": [
    "# T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36c1edc1-25e0-40b5-b39b-a6defda557ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run t-test \n",
    "all_cellline_TP_combis = [[\"HEK\",\"12\"],[\"HEK\",\"13\"],[\"HEK\",\"23\"],\n",
    "         [\"T98G\",\"12\"],[\"T98G\",\"13\"],[\"T98G\",\"23\"],\n",
    "         [\"PATU\",\"12\"],[\"PATU\",\"13\"],[\"PATU\",\"23\"],\n",
    "         [\"RPE\",\"12\"],[\"RPE\",\"13\"],[\"RPE\",\"23\"]]\n",
    "exclusionlist = WORKING_DIR + \"metainfo/common_essential_genes.tsv\"\n",
    "\n",
    "with open(WORKING_DIR + \"submission_script.sh\",\"w+\") as file:\n",
    "    file.write(f\"cd {WORKING_DIR}dLFC_analysis \\n\")\n",
    "    file.write(\"module load anaconda3/2021.05 \\n\")\n",
    "    file.write(\"source activate cas13_pipeline \\n\")\n",
    "    \n",
    "    for cellline,comparison in all_cellline_TP_combis:\n",
    "        \n",
    "        command = \" \".join([\"python\",WORKING_DIR+\"scripts/ttest.py\", #call script\n",
    "                    \"--workingdir\",WORKING_DIR, #passing working dir to script\n",
    "                    \"--cellline\",cellline, #path to counts files\n",
    "                    \"--comparison\",comparison, #comparison of timepoints\n",
    "                    #\"--exclusionlist\",exclusionlist,\n",
    "                   ])\n",
    "    \n",
    "        file.write(HPC_wrapper(command,\"medium\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
